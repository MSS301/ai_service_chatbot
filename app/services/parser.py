import fitz  # PyMuPDF
from pdf2image import convert_from_bytes
import pytesseract
from typing import List, Dict, Tuple
import re
from PIL import ImageFile
from app.core.logger import get_logger
from app.core.config import FORCE_OCR, OPENAI_API_KEY
from openai import OpenAI

# Enable loading truncated images
ImageFile.LOAD_TRUNCATED_IMAGES = True

logger = get_logger(__name__)

def _heuristic_shorten_heading(text: str) -> str:
    """
    Heuristic r√∫t g·ªçn ti√™u ƒë·ªÅ d√†i do OCR: b·ªè ƒëu√¥i d·∫•u ch·∫•m d·∫´n/s·ªë trang, l·∫•y ƒëo·∫°n √Ω ch√≠nh ƒë·∫ßu.
    """
    t = text.strip()
    # B·ªè ƒë∆∞·ªùng ch·∫•m v√† s·ªë trang ·ªü cu·ªëi: ....... 12
    t = re.sub(r'(\.{3,}\s*)?\d{1,3}\s*$', '', t).strip()
    # R√∫t ng·∫Øn theo c√¢u/d·∫•u ph√¢n c√°ch n·∫øu qu√° d√†i
    if len(t) > 200:
        m = re.search(r'([^.:\n]{10,200})(?:[.:\\n]|$)', t)
        if m:
            t = m.group(1).strip()
    t = re.sub(r'\s+', ' ', t)
    return t[:200].strip()

def _refine_heading_with_llm(kind: str, raw: str) -> str:
    """
    D√πng OpenAI ƒë·ªÉ r√∫t g·ªçn/chu·∫©n ho√° ti√™u ƒë·ªÅ ch∆∞∆°ng/b√†i khi ph√°t hi·ªán qu√° d√†i (nhi·ªÖu OCR).
    N·∫øu kh√¥ng c√≥ API key ho·∫∑c l·ªói, fallback v·ªÅ heuristic.
    """
    cleaned = _heuristic_shorten_heading(raw)
    if not OPENAI_API_KEY:
        return cleaned
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        prompt = (
            f"H√£y r√∫t g·ªçn v√† chu·∫©n ho√° ti√™u ƒë·ªÅ {kind} d∆∞·ªõi ƒë√¢y th√†nh m·ªôt d√≤ng ng·∫Øn g·ªçn, "
            f"lo·∫°i b·ªè ph·∫ßn d∆∞ nh∆∞ m√¥ t·∫£/c√¢u v√≠ d·ª•/s·ªë trang... Ch·ªâ tr·∫£ v·ªÅ CHU·ªñI TI√äU ƒê·ªÄ, kh√¥ng gi·∫£i th√≠ch.\n\n"
            f"Ti√™u ƒë·ªÅ g·ªëc:\n{raw}"
        )
        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "B·∫°n l√† b·ªô l·ªçc ti√™u ƒë·ªÅ. Ch·ªâ tr·∫£ v·ªÅ m·ªôt d√≤ng ti√™u ƒë·ªÅ s·∫°ch."},
                {"role": "user", "content": prompt},
            ],
            temperature=0,
        )
        content = (resp.choices[0].message.content or "").strip()
        content = re.sub(r'\s+', ' ', content)[:200].strip()
        return content or cleaned
    except Exception as e:
        logger.warning(f"LLM refine heading failed: {e}")
        return cleaned

def extract_toc_candidates(pages: List[Dict], max_scan_pages: int = 30) -> Dict[str, Dict] | tuple[Dict[str, Dict], str]:
    """
    Qu√©t ph·∫ßn M·ª§C L·ª§C (ho·∫∑c c√°c trang ƒë·∫ßu) ƒë·ªÉ l·∫•y danh s√°ch ch∆∞∆°ng v√† b√†i.
    Tr·∫£ v·ªÅ:
      - N·∫øu t√¨m th·∫•y vƒÉn b·∫£n m·ª•c l·ª•c r√µ r√†ng: (toc_dict, raw_toc_text)
      - N·∫øu kh√¥ng: toc_dict (fallback d·ª±a v√†o headings ƒë√£ detect)

    toc_dict format:
    {
      "Ch∆∞∆°ng I. ...": {
        "lessons": [{"title": "B√†i 1. ...", "page": 5}, ...],
        "chapter_first_page": 5
      },
      ...
    }
    """
    toc: Dict[str, Dict] = {}
    text_accum = []
    for p in pages[:max_scan_pages]:
        text_accum.append(p.get("text", ""))
    head_text = "\n".join(text_accum)

    # C·∫Øt ri√™ng khu v·ª±c c√≥ th·ªÉ l√† M·ª§C L·ª§C
    # T√¨m t·ª´ "M·ª§C L·ª§C" v√† l·∫•y ~2000 k√Ω t·ª± sau ƒë√≥, n·∫øu c√≥
    m = re.search(r'M·ª§C\s*L·ª§C', head_text, re.IGNORECASE)
    candidate_text = head_text
    if m:
        start = m.start()
        candidate_text = head_text[start:start+8000]

    # Chu·∫©n ho√° xu·ªëng t·ª´ng d√≤ng ƒë·ªÉ regex
    lines = [l.strip() for l in candidate_text.splitlines() if l.strip()]

    current_chapter = None
    i = 0
    while i < len(lines):
        line = lines[i]

        # D√≤ng ch∆∞∆°ng (c√≥ th·ªÉ ƒëa d√≤ng)
        ch = re.match(r'^(Ch∆∞∆°ng|CH∆Ø∆†NG|Ph·∫ßn|PH·∫¶N)\s+([IVXLCDM\d]+)[\.\s]+(.{2,})$', line)
        if ch:
            num = ch.group(2)
            title = ch.group(3).strip()
            # G·ªôp th√™m 1-2 d√≤ng ti·∫øp theo n·∫øu l√† ph·∫ßn ti·∫øp c·ªßa ti√™u ƒë·ªÅ ch∆∞∆°ng (th∆∞·ªùng to√†n ch·ªØ hoa/kho·∫£ng tr·∫Øng)
            j = i + 1
            join_parts = [title]
            while j < len(lines) and j <= i + 3:
                nxt = lines[j]
                if re.match(r'^(B√†i|B√ÄI)\s+\d+', nxt) or re.match(r'^(Ch∆∞∆°ng|CH∆Ø∆†NG|Ph·∫ßn|PH·∫¶N)\s+', nxt) or "HO·∫†T ƒê·ªòNG" in nxt.upper():
                    break
                # D√≤ng to√†n ch·ªØ/space ho·∫∑c qu√° ng·∫Øn ƒë∆∞·ª£c xem l√† ti·∫øp ti√™u ƒë·ªÅ
                if re.match(r'^[A-Za-z√Ä-·ª¥√†-·ªπ0-9\\s\\.,]+$', nxt) or len(nxt) <= 40:
                    join_parts.append(nxt.strip())
                    j += 1
                else:
                    break
            title = " ".join(join_parts)
            title = re.sub(r'(\.{3,}\s*)?\d{1,3}$', '', title).strip()
            chapter_title = f"{ch.group(1).capitalize()} {num}. {title}".strip()
            if chapter_title not in toc:
                toc[chapter_title] = {"lessons": [], "chapter_first_page": None}
            current_chapter = chapter_title
            i = j
            continue

        # D√≤ng b√†i h·ªçc (c√≥ th·ªÉ ƒëa d√≤ng, s·ªë trang c√≥ th·ªÉ ·ªü d√≤ng sau)
        le = re.match(r'^(B√†i|B√ÄI)\s+(\d+)[\.\s]+(.+)$', line)
        if le and current_chapter:
            title_main = le.group(3).strip()
            parts = [title_main]
            page_no = None
            j = i + 1
            while j < len(lines) and j <= i + 3:
                nxt = lines[j].strip()
                # N·∫øu l√† s·ªë trang ƒë·ª©ng ri√™ng ·ªü d√≤ng ti·∫øp theo
                if re.match(r'^\d{1,3}$', nxt):
                    page_no = int(nxt)
                    j += 1
                    break
                # N·∫øu g·∫∑p b·∫Øt ƒë·∫ßu ch∆∞∆°ng/b√†i m·ªõi th√¨ d·ª´ng
                if re.match(r'^(B√†i|B√ÄI)\s+\d+', nxt) or re.match(r'^(Ch∆∞∆°ng|CH∆Ø∆†NG|Ph·∫ßn|PH·∫¶N)\s+', nxt) or "HO·∫†T ƒê·ªòNG" in nxt.upper():
                    break
                # Ng∆∞·ª£c l·∫°i, n·ªëi ti·∫øp ti√™u ƒë·ªÅ b√†i
                parts.append(nxt)
                j += 1
            lesson_title = " ".join(parts)
            lesson_title = re.sub(r'(\.{3,}\s*)?\d{1,3}$', '', lesson_title).strip()
            # B·ªè qua m·ª•c kh√¥ng ph·∫£i b√†i h·ªçc ch√≠nh
            if lesson_title.lower().startswith("b√†i t·∫≠p cu·ªëi") or "ho·∫°t ƒë·ªông" in lesson_title.lower() or "b·∫£ng tra" in lesson_title.lower() or "gi·∫£i th√≠ch thu·∫≠t ng·ªØ" in lesson_title.lower():
                i = j
                continue
            toc[current_chapter]["lessons"].append({
                "title": f"B√†i {le.group(2)}. {lesson_title}",
                "page": page_no
            })
            if toc[current_chapter]["chapter_first_page"] is None and page_no:
                toc[current_chapter]["chapter_first_page"] = page_no
            i = j
            continue

        i += 1

    # Fallback: n·∫øu kh√¥ng th·∫•y M·ª§C L·ª§C, d·ª±a v√†o headings ƒë√£ detect
    if not toc:
        tmp: Dict[str, Dict] = {}
        for p in pages[:max_scan_pages]:
            ch = p.get("chapter") or ""
            le = p.get("lesson") or ""
            if ch:
                tmp.setdefault(ch, {"lessons": [], "chapter_first_page": p.get("page_num")})
            if ch and le and all(l["title"] != le for l in tmp[ch]["lessons"]):
                tmp[ch]["lessons"].append({"title": le, "page": p.get("page_num")})
        toc = tmp
        return toc

    # C√≥ m·ª•c l·ª•c: tr·∫£ v·ªÅ c·∫£ raw_text ƒë·ªÉ LLM t√°i c·∫•u tr√∫c d√≤ng d√†i/ng·∫Øt d√≤ng
    return toc, candidate_text
def _has_text_layer(doc: fitz.Document) -> bool:
    pages = min(3, len(doc))
    for i in range(pages):
        if doc[i].get_text().strip():
            return True
    return False

def _clean_text(text: str) -> str:
    """Clean v√† normalize text"""
    # Remove excessive whitespace
    text = re.sub(r'\s+', ' ', text)
    # Remove weird characters
    text = re.sub(r'[<>]+', '', text)
    return text.strip()

def _detect_chapter_info(text: str, page_num: int) -> Tuple[str, str]:
    """
    Ph√°t hi·ªán ch∆∞∆°ng v√† b√†i t·ª´ text
    
    SGK Vi·ªát Nam patterns:
    - CH∆Ø∆†NG I. T√äN CH∆Ø∆†NG
    - CH∆Ø∆†NG 1. T√äN CH∆Ø∆†NG  
    - B√†i 1. T√™n b√†i
    - B√ÄI 2. T√äN B√ÄI
    
    Returns: (chapter_name, lesson_name)
    """
    chapter_name = ""
    lesson_name = ""
    
    # Clean text tr∆∞·ªõc
    text = _clean_text(text)
    
    # ============ DETECT CHAPTER ============
    # Pattern 1: "CH∆Ø∆†NG I. T√äN" (ch·ªØ hoa, s·ªë La M√£)
    # Stop at B√†i, CH∆Ø∆†NG, etc.
    match = re.search(
        r'CH∆Ø∆†NG\s+([IVXLCDM]+)[\.\s]+(.+?)(?=\s+(?:B√†i|B√ÄI|Ch∆∞∆°ng|CH∆Ø∆†NG|B√†i t·∫≠p|HO·∫†T|\d+\s*$)|$)',
        text,
        re.IGNORECASE | re.DOTALL | re.MULTILINE
    )
    if match:
        num = match.group(1).upper()
        title = match.group(2).strip()
        # Validate: must have meaningful title (not just "B√†i t·∫≠p cu·ªëi")
        if title and len(title) > 3 and not title.startswith("t·∫≠p"):
            title = re.sub(r'\s*\d+\s*$', '', title)  # Remove page numbers at end
            chapter_name = f"Ch∆∞∆°ng {num}. {title}"
            logger.debug(f"Page {page_num}: Detected chapter (Roman): '{chapter_name}'")
    
    # Pattern 2: "Ch∆∞∆°ng 1. T√™n" (ch·ªØ th∆∞·ªùng, s·ªë ·∫¢-r·∫≠p)
    if not chapter_name:
        match = re.search(
            r'Ch∆∞∆°ng\s+(\d+)[\.\s]+(.+?)(?=\s+(?:B√†i|B√ÄI|Ch∆∞∆°ng|CH∆Ø∆†NG|B√†i t·∫≠p|HO·∫†T|\d+\s*$)|$)',
            text,
            re.IGNORECASE | re.DOTALL | re.MULTILINE
        )
        if match:
            num = match.group(1)
            title = match.group(2).strip()
            if title and len(title) > 3 and not title.startswith("t·∫≠p"):
                title = re.sub(r'\s*\d+\s*$', '', title)
                chapter_name = f"Ch∆∞∆°ng {num}. {title}"
                logger.debug(f"Page {page_num}: Detected chapter (Arabic): '{chapter_name}'")
    
    # Pattern 3: "PH·∫¶N I. T√äN" (m·ªôt s·ªë SGK d√πng "ph·∫ßn" thay v√¨ "ch∆∞∆°ng")
    if not chapter_name:
        match = re.search(
            r'PH·∫¶N\s+([IVXLCDM\d]+)[\.\s]+(.+?)(?=\s+(?:B√†i|B√ÄI|Ch∆∞∆°ng|CH∆Ø∆†NG|B√†i t·∫≠p|HO·∫†T|\d+\s*$)|$)',
            text,
            re.IGNORECASE | re.DOTALL | re.MULTILINE
        )
        if match:
            num = match.group(1)
            title = match.group(2).strip()
            if title and len(title) > 3 and not title.startswith("t·∫≠p"):
                title = re.sub(r'\s*\d+\s*$', '', title)
                chapter_name = f"Ph·∫ßn {num}. {title}"
                logger.debug(f"Page {page_num}: Detected chapter (Ph·∫ßn): '{chapter_name}'")
    
    # ============ DETECT LESSON ============
    # Pattern 1: "B√ÄI 1. T√äN B√ÄI" (ch·ªØ hoa)
    # Stop at page numbers or new lines with digits
    match = re.search(
        r'B√ÄI\s+(\d+)[\.\s]+([^\n]*?)(?=\s+\d+\s*$|\n\s*\d+\s*$|$)',
        text,
        re.IGNORECASE
    )
    if match:
        num = match.group(1)
        title = match.group(2).strip()
        # Clean up title
        title = re.sub(r'\s*\d+\s*$', '', title)  # Remove trailing page number
        title = re.sub(r'^\W+|\W+$', '', title)  # Remove leading/trailing punctuation
        lesson_name = f"B√†i {num}. {title}"
        logger.debug(f"Page {page_num}: Detected lesson: '{lesson_name}'")
    
    # Pattern 2: "B√†i h·ªçc 1. T√™n" (m·ªôt s·ªë SGK)
    if not lesson_name:
        match = re.search(
            r'B√†i\s+h·ªçc\s+(\d+)[\.\s]+([^\n]*?)(?=\s+\d+\s*$|\n\s*\d+\s*$|$)',
            text,
            re.IGNORECASE
        )
        if match:
            num = match.group(1)
            title = match.group(2).strip()
            title = re.sub(r'\s*\d+\s*$', '', title)
            lesson_name = f"B√†i {num}. {title}"
            logger.debug(f"Page {page_num}: Detected lesson (alt): '{lesson_name}'")
    
    # Pattern 3: "¬ß1. T√™n" (k√Ω hi·ªáu ƒëo·∫°n)
    if not lesson_name:
        match = re.search(
            r'¬ß\s*(\d+)[\.\s]+([^\n]*?)(?=\s+\d+\s*$|\n\s*\d+\s*$|$)',
            text
        )
        if match:
            num = match.group(1)
            title = match.group(2).strip()
            title = re.sub(r'\s*\d+\s*$', '', title)
            lesson_name = f"B√†i {num}. {title}"
            logger.debug(f"Page {page_num}: Detected lesson (¬ß): '{lesson_name}'")
    
    # Normalize/Refine lengths: d√πng LLM khi qu√° d√†i (nghi nhi·ªÖu), fallback heuristic
    MAX_LEN = 200
    if len(chapter_name) > MAX_LEN:
        chapter_name = _refine_heading_with_llm("ch∆∞∆°ng", chapter_name)
        logger.info(f"Page {page_num}: Chapter refined")
    
    if len(lesson_name) > MAX_LEN:
        lesson_name = _refine_heading_with_llm("b√†i", lesson_name)
        logger.info(f"Page {page_num}: Lesson refined")
    
    return chapter_name, lesson_name

def _extract_text_with_structure(page: fitz.Page, page_num: int) -> Tuple[str, str, str]:
    """
    Tr√≠ch xu·∫•t text + detect structure t·ª´ 1 page
    
    Strategy:
    1. Get full text
    2. Get blocks ƒë·ªÉ t√¨m headings (font size l·ªõn)
    3. Combine headings + text ‚Üí detect chapter/lesson
    """
    # Get full text
    text = page.get_text("text")
    
    # Get blocks ƒë·ªÉ ph√¢n t√≠ch font
    blocks = page.get_text("dict").get("blocks", [])
    
    # Extract text t·ª´ blocks c√≥ font size l·ªõn (likely headings)
    heading_texts = []
    for block in blocks:
        if block.get("type") == 0:  # Text block
            for line in block.get("lines", []):
                for span in line.get("spans", []):
                    font_size = span.get("size", 0)
                    span_text = span.get("text", "").strip()
                    
                    # Headings th∆∞·ªùng c√≥ font >12pt v√† √≠t nh·∫•t 5 k√Ω t·ª±
                    if font_size >= 12 and len(span_text) >= 5:
                        heading_texts.append(span_text)
    
    # Combine headings v·ªõi ƒë·∫ßu text (2000 chars ƒë·∫ßu th∆∞·ªùng ch·ª©a chapter/lesson)
    search_text = "\n".join(heading_texts) + "\n" + text[:2000]
    
    # Detect chapter/lesson
    chapter, lesson = _detect_chapter_info(search_text, page_num)
    
    return text, chapter, lesson

def parse_pdf_bytes(pdf_bytes: bytes, lang: str = "vie", prefer_text: bool = True) -> List[Dict]:
    """
    Parse PDF v·ªõi improved structure detection
    
    Key improvements:
    1. Better regex patterns cho SGK Vi·ªát Nam
    2. Validate chapter/lesson lengths
    3. Clean text (remove <>, excessive spaces)
    4. Debug logging
    
    Returns: List[Dict] v·ªõi keys:
        - page_num: int
        - text: str
        - blocks: List (raw block data)
        - chapter: str (e.g., "Ch∆∞∆°ng I. ·ª®NG D·ª§NG ƒê·∫†O H√ÄM...")
        - lesson: str (e.g., "B√†i 1. T√≠nh ƒë∆°n ƒëi·ªáu v√† c·ª±c tr·ªã c·ªßa h√†m s·ªë")
    """
    doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    pages = []
    
    # State ƒë·ªÉ "carry forward"
    current_chapter = ""
    current_lesson = ""
    
    try:
        should_text = prefer_text and _has_text_layer(doc) and not FORCE_OCR
        
        if should_text:
            logger.info(f"Parsing {len(doc)} pages with TEXT layer")
            
            for i in range(len(doc)):
                page = doc[i]
                text, detected_chapter, detected_lesson = _extract_text_with_structure(page, i+1)
                
                # Update state khi detect ƒë∆∞·ª£c
                if detected_chapter:
                    current_chapter = detected_chapter
                    logger.info(f"üìò Page {i+1}: Chapter = '{current_chapter}'")
                
                if detected_lesson:
                    current_lesson = detected_lesson
                    logger.info(f"üìó Page {i+1}: Lesson = '{current_lesson}'")
                
                pages.append({
                    "page_num": i + 1,
                    "text": text,
                    "blocks": page.get_text("dict").get("blocks", []),
                    "chapter": current_chapter,
                    "lesson": current_lesson
                })
        
        else:
            logger.info(f"Parsing {len(doc)} pages with OCR")
            images = convert_from_bytes(pdf_bytes, dpi=300)
            
            for i, img in enumerate(images):
                try:
                    txt = pytesseract.image_to_string(img, lang=lang)
                    detected_chapter, detected_lesson = _detect_chapter_info(txt[:2000], i+1)
                    
                    if detected_chapter:
                        current_chapter = detected_chapter
                        logger.info(f"üìò Page {i+1}: Chapter = '{current_chapter}'")
                    
                    if detected_lesson:
                        current_lesson = detected_lesson
                        logger.info(f"üìó Page {i+1}: Lesson = '{current_lesson}'")
                    
                    pages.append({
                        "page_num": i + 1,
                        "text": txt,
                        "blocks": [],
                        "chapter": current_chapter,
                        "lesson": current_lesson
                    })
                except OSError as e:
                    logger.warning(f"‚ö†Ô∏è Page {i+1}: Image truncated or corrupted, skipping OCR. Error: {e}")
                    # Fallback: try to extract text directly from PDF if possible
                    try:
                        page = doc[i]
                        txt = page.get_text()
                        detected_chapter, detected_lesson = _detect_chapter_info(txt[:2000], i+1)
                        
                        if detected_chapter:
                            current_chapter = detected_chapter
                        if detected_lesson:
                            current_lesson = detected_lesson
                        
                        pages.append({
                            "page_num": i + 1,
                            "text": txt,
                            "blocks": page.get_text("dict").get("blocks", []),
                            "chapter": current_chapter,
                            "lesson": current_lesson
                        })
                    except Exception as e2:
                        logger.error(f"‚ùå Page {i+1}: Failed to extract text (OCR and PDF both failed). Error: {e2}")
                        # Add empty page to maintain page numbering
                        pages.append({
                            "page_num": i + 1,
                            "text": "",
                            "blocks": [],
                            "chapter": current_chapter,
                            "lesson": current_lesson
                        })
                except Exception as e:
                    logger.error(f"‚ùå Page {i+1}: Unexpected error during OCR. Error: {e}")
                    # Add empty page to maintain page numbering
                    pages.append({
                        "page_num": i + 1,
                        "text": "",
                        "blocks": [],
                        "chapter": current_chapter,
                        "lesson": current_lesson
                    })
        
        # Summary logging
        unique_chapters = {p["chapter"] for p in pages if p["chapter"]}
        unique_lessons = {p["lesson"] for p in pages if p["lesson"]}
        
        logger.info("‚úÖ Parse complete:")
        logger.info(f"   - {len(pages)} pages")
        logger.info(f"   - {len(unique_chapters)} chapters: {list(unique_chapters)[:3]}")
        logger.info(f"   - {len(unique_lessons)} lessons: {list(unique_lessons)[:3]}")
        
        # Warning n·∫øu detection rate qu√° th·∫•p
        with_chapter_pct = len([p for p in pages if p["chapter"]]) / len(pages) * 100
        with_lesson_pct = len([p for p in pages if p["lesson"]]) / len(pages) * 100
        
        if with_chapter_pct < 30:
            logger.warning(f"‚ö†Ô∏è  Low chapter detection: {with_chapter_pct:.1f}% pages")
        if with_lesson_pct < 20:
            logger.warning(f"‚ö†Ô∏è  Low lesson detection: {with_lesson_pct:.1f}% pages")
        
        return pages
    
    finally:
        doc.close()
